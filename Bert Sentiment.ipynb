{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6b454b",
   "metadata": {},
   "source": [
    "this is just the setup to fine tune the BERT uncased model on the Sentiment 140 dataset. I could have gone with a classic ML model (TF-IDF, Logistic Regression/Naive Bayes) for quick training, but I wanted a bit of a challenge. \n",
    "Make sure deps is installed: \n",
    "#pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31812c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# only loading the first 100k rows from the sentiment 140 dataset for faster training on Colab.\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",\n",
    "                  encoding= \"latin-1\", \n",
    "                  names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"], \n",
    "                  nrows=100000\n",
    ")\n",
    "\n",
    "# map the labels. Target: 0 is negative, 4 is positive. remap to 0/1 binary \n",
    "df[\"label\"]= df[\"target\"].replace({0:0, 4:1})\n",
    "\n",
    "# Converting to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label\"]])\n",
    "\n",
    "# Split between testing and training sets\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb909aa",
   "metadata": {},
   "source": [
    "Now, I'm going to tokenize the training and testing datasets. This means I'm breaking down the raw data (text) into smaller, fundamental units (tokens) that the AI model can understand. For text like the Sentiment 140 dataset, it means converting words, subwords and characters into numerical representations. This is the actual boilerplate, but the HuggingFace code block is actually 8 lines, lol. This is something I had to look up\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#Load the dataset\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"latin-1\", header=None)\n",
    "df = df[[0, 5]]  # sentiment label = col 0, tweet = col 5\n",
    "df.columns = [\"label\", \"text\"]\n",
    "\n",
    "#Convert labels: 0 = negative, 4 = positive â†’ map to binary\n",
    "df['label'] = df['label'].map({0: 0, 4: 1})\n",
    "\n",
    "#Sample smaller dataset if needed\n",
    "df = df.sample(100000, random_state=42)\n",
    "\n",
    "#Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "#Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#Tokenize\n",
    "train_encodings = tokenizer(\n",
    "    list(train_texts),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=64\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    list(val_texts),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=64\n",
    ")\n",
    "\n",
    "#Wrap into torch Dataset\n",
    "import torch\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset = TweetDataset(val_encodings, val_labels)\n",
    "\n",
    "print(train_dataset[0])  # preview one sample\n",
    "\n",
    "More Notes: \n",
    "the function below gets the text column from the dataset, then pads everything to the model's max sequence length (default 512), and shortens the tweets longer than the limit. Now, each row becomes a dictionary with input_ids (the tokens) and the attention mask (binary 1 and 0s). \n",
    "\n",
    "then the function is applied to the entire dataset. batched=True runs the function in chunks instead of rows at a time, to save time. \n",
    "\n",
    "then the dataset is turned into Pytorch tensors (multidimensional arrays) with only the columns the model needs. Input id(tokenized tweet), attention mask (which part is padding or real) and label (sentiment class 0,1,2,3,4 which is what I set) This is the Pytorch format. \n",
    "\n",
    "for the sentiment140 dataset, clean up the dataset (get rid of the weird characters, hashtags, mentions), tokenize each tweet to get the input_ids and attention masks, then pair the tokenized tweets with their labels (0=negative, 1=positive) Simple! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc86838",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e654af",
   "metadata": {},
   "source": [
    "Now, i'm going to load the dataset and pray for the best. Pretty self explanatory. \n",
    "the first import is the correct model class set up for the classification task at hand. \n",
    "from pretrained loads the pretrained weights from bert_base_uncased (uncased defintition I got from the huggingface site)\n",
    "num_labels=2 tells the model that this is a binary classification task. I can set it to 3, if I want neutral classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18844a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8907c",
   "metadata": {},
   "source": [
    "Now, train the model on the 100k rows of data. This should only take about 2 hours (max) to train, based on the free tier of HgFc Colab, which is about 16GB of VRAM. You get more with paid tiers, of course. The GPU is a Tesla t4. \n",
    "\n",
    "Note: Epoch is a complete pass through the training dataset. 2 epochs mean the model will passthrough or \"see\" all 100k tweets twice. Each pass helps the model \"learn\" a bit more. In my case, 2 passes is enough, anymore would cause overfitting (memorizing the training data instead of generalizing it). On the free tier of Colab, 2 or 3 passes is enough. \n",
    "\n",
    "Note: \"evaluation_strategy = epoch\" tells HuggingFace when to evaluate on the test set. epoch means evaluate once at the end of each epoch (passthrough), steps means evaluate every X training steps (in this case, i've set it to 500). This helps track if the model is improving or if it's overfitting. \n",
    "\n",
    "Note: \"per_device_train_batch_size\" is \"how many tweets does the model read before it updates the weights?\" in this case, its 16 tweets, since I set the size to 16. If I set it higher, I would need more VRAM, if I set it too small, then training takes longer. Looks like if you set the batch size to the VRAM size, you're good. \n",
    "\n",
    "Note: batch_size x sequence length x model size determines how much VRAM I will need. BERT is 110M parameters + sequence length 128 + batch size 16 all fits on a T4. You could do a batch size 64, but you may run out of memory. (OOM) 128 for a seq length is the max, 512 is possible but good luck. \n",
    "\n",
    "Note: if you overfit and get the OOM message, just clear the GPU memory without starting the entire runtime. \n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "Note: weight decay is a regularization trick to prevent overfitting. \n",
    "\n",
    "Note: can use Tensorboard to track training logs\n",
    "\n",
    "Note: the trainer block wraps all of this into a HuggingFace trainer object. this handles the training loop, eval, checkpoint saving, logging, gpu handling and save me from writing more lines of code. Thank gosh. \n",
    "\n",
    "So, what happens? Each epoch loads a batch of tweets, runs them through BERT, computes loss (prediction vs label), backpropagates and updates weights(an algorithm that uses the chain rule to compute the gradient of the loss function with respect to the network's weights and biases. Also known as a backward pass and it allows the network to adjust parameters to reduce errors, improve prediction by spreading the error from the output later backward to the input layer. Put it in reverse, Terry, to learn from errors and give better predictions.) and then logs progress. It's one of the coolest things, a wrapper around Pytorch so I don't have to write the dreaded manual loop. \n",
    "\n",
    "Note: Weight decay. A type of regularization (L2) with the goal of preventing overfitting. Overfitting is when the model memorizes training data instead of learning the general pattern(s) from it. It adds a penalty term to the loss function, so the model will go after the simpler (the loss function without the penalty) solutions. Every update, it shrinks the weights towards zero. Loss = Original Loss + lambda * sum of squared weights. lambda in this equation is the regularization strength (hyperparameter). It's a fine tuning thing. In my code, the 0.01 represents a small pull towards zero with each update. It's a typical value. It makes the larger weights too costly, so the model goes with the generalizations that aren't so costly. \n",
    "weight update = current weight - learning weight * (gradient of error + lambda * weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Training Arguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "     output_dir=\"./results\", \n",
    "     eval_strategy=\"epoch\", \n",
    "     learning_rate=2e-5,\n",
    "     per_device_train_batch_size=16,\n",
    "     per_device_eval_batch_size=16,\n",
    "     num_train_epochs=2,\n",
    "     weight_decay=0.01,\n",
    "     logging_dir=\"./logs\",\n",
    "     logging_steps=500,\n",
    "     save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.labels_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds), \"f1\": f1_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "     model=model, \n",
    "     args=training_args, \n",
    "     train_dataset=train_ds, \n",
    "     eval_dataset=test_ds, \n",
    "     tokenizer=tokenizer, \n",
    "     compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c756e",
   "metadata": {},
   "source": [
    "now, save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"C:\\Users\\khowi\\Desktop\\SentimentAn/bert-sentiment\")\n",
    "tokenizer.save_pretrained(\"C:\\Users\\khowi\\Desktop\\SentimentAn/bert-sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
